{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Gradient for sparse linear systems\n",
    "\n",
    "Author: Alexandre Gramfort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 1\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "w = np.random.randn(n_features)\n",
    "b = 10.\n",
    "y = np.dot(X, w) + b\n",
    "y += 0.3 * np.random.randn(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.csr_matrix(X)  # make X sparse\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define a linear operator that implements a matrix vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import LinearOperator\n",
    "\n",
    "alpha = 0.  # the regularization parameter\n",
    "\n",
    "def matvec(w):\n",
    "    \"\"\"Define the matrix vector product with X.T @ X + alpha Id\"\"\"\n",
    "    return X.T.dot(X.dot(w)) + alpha * w\n",
    "    \n",
    "A = LinearOperator((n_features, n_features), matvec=matvec, dtype=X.dtype)\n",
    "\n",
    "# A now behaves as matrix that can be multiplied by a vector\n",
    "A @ np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use it to solve the Ridge regression problem with conjugate gradient.\n",
    "\n",
    "What we need to solve is the following linear system:\n",
    "\n",
    "$$(X^\\top X + \\alpha I) w = X^\\top y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_ridge(X, y, alpha=0., x0=None):\n",
    "    n_features = X.shape[1]\n",
    "    # matvec = lambda w: X.T.dot(X.dot(w)) + alpha * w\n",
    "    def matvec(w):\n",
    "        return X.T.dot(X.dot(w)) + alpha * w\n",
    "    A = LinearOperator((n_features, n_features), \n",
    "                       matvec=matvec, dtype=X.dtype)\n",
    "    w_hat, info = sparse.linalg.cg(A, X.T.dot(y), x0=x0)\n",
    "    return w_hat\n",
    "\n",
    "alpha = 0.  # the regularization parameter\n",
    "w_hat = sparse_ridge(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(w, b=0.):\n",
    "    plt.plot(X.toarray()[:, 0], y, 'o', alpha=0.2)\n",
    "\n",
    "    xx = np.linspace(-2, 2, 100)\n",
    "    yy = np.dot(xx[:, np.newaxis], w) + b\n",
    "    plt.plot(xx, yy, 'k')\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_data(w_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that we have a problem with the regularization of the intercept.\n",
    "\n",
    "Here are two ways to fix it.\n",
    "\n",
    "First we will not regualize at all the intercept. We need to\n",
    "create a new matrix that contains the last columns with ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb = sparse.hstack((X, np.ones((n_samples, 1))))\n",
    "\n",
    "def sparse_ridge_intercept(Xb, y, alpha=0., x0=None):\n",
    "    n_features = Xb.shape[1] - 1\n",
    "    def matvec(w):\n",
    "        alpha_diag = np.zeros(n_features + 1)\n",
    "        alpha_diag[:n_features] = alpha\n",
    "        return Xb.T.dot(Xb.dot(w)) + np.diag(alpha_diag) @ w\n",
    "    A = LinearOperator((n_features + 1, n_features + 1), \n",
    "                       matvec=matvec, dtype=Xb.dtype)\n",
    "    (w_hat, b_hat), info = sparse.linalg.cg(A, Xb.T.dot(y), x0=x0)\n",
    "    return w_hat, b_hat\n",
    "\n",
    "alpha = 100.  # the regularization parameter\n",
    "w_hat, b_hat = sparse_ridge_intercept(Xb, y, alpha=alpha, x0=[0, 0])\n",
    "\n",
    "plot_data(w_hat, b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: try to not regularize the intercept too much.\n",
    "For this the idea is to add a columns filled with a constant value.\n",
    "If this value is 100 the intercept will be 100 times less regularized\n",
    "that if it was filled with ones. This is related to the parameter\n",
    "`intercept_scaling` in the [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb = sparse.hstack((X, 100 * np.ones((n_samples, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 100.  # regularization parameter lambda\n",
    "w_hat, b_hat = sparse_ridge(Xb, y, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(w_hat, b_hat * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some \"Big Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = 10000, 1000000\n",
    "density = 1e-5\n",
    "\n",
    "rng1 = np.random.RandomState(42)\n",
    "rng2 = np.random.RandomState(43)\n",
    "\n",
    "nnz = int(n_samples*n_features*density)\n",
    "\n",
    "row = rng1.randint(n_samples, size=nnz)\n",
    "cols = rng2.randint(n_features, size=nnz)\n",
    "data = rng1.rand(nnz)\n",
    "\n",
    "X = sparse.coo_matrix((data, (row, cols)), shape=(n_samples, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(n_features)\n",
    "y = X.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat = sparse_ridge(X, y, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The benefits of warm start (providing a good init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sparse_ridge(X, y, alpha=0.01, x0=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sparse_ridge(X, y, alpha=0.02, x0=w_hat)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
